---
authors:
- Chi-Hua Wang
- admin
- Yang Yu
- Botao Hao
- Guang Cheng
date: "2021-01-23T00:00:00Z"
doi: ""
featured: true
projects:
- exploration-bandit
publication: "*Preprint*"
publication_short: ""
publication_types:
- "3"
publishDate: "2021-01-23T00:00:00Z"
summary: In this paper, we propose a novel data-driven exploration policy in bandit algorithms with bounded or unbounded rewards, called residual bootstrap exploration (\texttt{ReBoot}). The \texttt{ReBoot} enforces exploration by injecting data-driven randomness through a residual-based history randomization mechanism. This novel mechanism captures the underlying distributional properties of fitting errors, and more importantly boosts exploration to escape from suboptimal solutions (for small sample sizes) by assisting exploration level in an \textit{unconventional} way. In theory, with appropriate exploration assistance, \texttt{ReBoot} provably secures instance-dependent logarithmic regret in sub-Gaussian multi-armed bandits. We evaluate the \texttt{ReBoot} in different synthetic multi-armed bandits problems and observe that \texttt{ReBoot} is comparable to the Thompson sampling method.
tags:
- publications
- bandit algorithms
- nonparametric statistics
- uncertainty quantification
title: Residual Bootstrap Exploration for Bandit Algorithms
url_pdf: https://arxiv.org/pdf/2002.08436.pdf
---